{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##FashionMNIST 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 784), y_train shape: (60000,)\n",
      "x_test shape: (10000, 784), y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 데이터 로드 함수\n",
    "def load_fashion_mnist_from_csv(train_path, test_path):\n",
    "    # CSV 파일 읽기\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "    # 라벨과 이미지 데이터 분리\n",
    "    y_train = train_df['label'].values  # 라벨\n",
    "    x_train = train_df.drop(columns=['label']).values  # 이미지 데이터\n",
    "\n",
    "    y_test = test_df['label'].values  # 라벨\n",
    "    x_test = test_df.drop(columns=['label']).values  # 이미지 데이터\n",
    "\n",
    "    # 데이터 형태 변환 및 정규화\n",
    "    x_train = x_train.astype(np.float32) / 255.0  # 0~1 스케일로 정규화\n",
    "    x_test = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "# FashionMNIST 데이터 로드\n",
    "train_path = \"data/fashion-mnist_train.csv\"\n",
    "test_path = \"data/fashion-mnist_test.csv\"\n",
    "(x_train, y_train), (x_test, y_test) = load_fashion_mnist_from_csv(train_path, test_path)\n",
    "\n",
    "# 데이터 형태 출력\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "# from torchvision import datasets, transforms\n",
    "\n",
    "# 손실 함수 정의 (기존 함수 그대로 사용)\n",
    "def Regularized_loss(model, n, y_pred, y_true, p=4, lam_margin=0.01, lam_l1=0.001, lam_l2=0.001):\n",
    "    classification_loss = -torch.mean(y_true * torch.log_softmax(y_pred, dim=1))\n",
    "    last_layer_weight = model.network[-1].weight\n",
    "    RG_loss_margin = 1/n * torch.norm(\n",
    "        last_layer_weight.unsqueeze(1) - last_layer_weight.unsqueeze(0), p=2, dim=2\n",
    "    ).pow(p).sum()\n",
    "    RG_loss_regularization = lam_l1 * torch.norm(last_layer_weight, p=1) + \\\n",
    "                             lam_l2 * torch.norm(last_layer_weight, p=2)\n",
    "    RG_loss = lam_margin * RG_loss_margin + RG_loss_regularization\n",
    "    loss = classification_loss + RG_loss\n",
    "    return loss\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.3):\n",
    "        super(MLP, self).__init__()\n",
    "        self.network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_rate),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_rate),\n",
    "            torch.nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# 가중치 초기화 함수\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "# Early Stopping 클래스 정의\n",
    "class EarlyStoppingAccuracy:\n",
    "    def __init__(self, patience=10, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_acc = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, current_acc):\n",
    "        if self.best_acc is None:\n",
    "            self.best_acc = current_acc\n",
    "        elif current_acc - self.best_acc > self.min_delta:\n",
    "            self.best_acc = current_acc\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_MLR_with_FashionMNIST(x_train, y_train, x_test, y_test, para):\n",
    "    # Numpy 데이터를 PyTorch Tensor로 변환 및 정규화\n",
    "    x_train = x_train.astype(np.float32) / 255.0  # 정규화 (0~1 스케일)\n",
    "    x_test = x_test.astype(np.float32) / 255.0\n",
    "    x_train = x_train.reshape(-1, 28 * 28)  # Flatten\n",
    "    x_test = x_test.reshape(-1, 28 * 28)\n",
    "\n",
    "    # 라벨을 Tensor로 변환\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    # 데이터셋 생성\n",
    "    train_dataset = TensorDataset(torch.tensor(x_train), y_train)\n",
    "    test_dataset = TensorDataset(torch.tensor(x_test), y_test)\n",
    "\n",
    "    # DataLoader 생성\n",
    "    train_loader = DataLoader(train_dataset, batch_size=para[\"batch_size\"], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=para[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    # MLP 모델 초기화\n",
    "    input_dim = 28 * 28  # FashionMNIST 이미지 크기\n",
    "    hidden_dim = para.get(\"hidden_dim\", 512)\n",
    "    dropout_rate = para.get(\"dropout_rate\", 0.3)\n",
    "    model = MLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=10, dropout_rate=dropout_rate)\n",
    "    model.apply(initialize_weights)\n",
    "\n",
    "    # 옵티마이저와 학습 스케줄러 설정\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=para[\"lr\"], weight_decay=para[\"weight_decay\"])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "    # Early Stopping 설정\n",
    "    early_stopping = EarlyStoppingAccuracy(patience=para[\"patience\"], min_delta=para[\"min_delta\"])\n",
    "\n",
    "    # 학습 루프\n",
    "    for epoch in range(para[\"num_epoch\"]):\n",
    "        print(f\"Epoch {epoch + 1}/{para['num_epoch']} starting...\")\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            y_onehot = F.one_hot(y_batch, num_classes=10).float()\n",
    "            loss = Regularized_loss(model, len(x_train), y_pred, y_onehot, para[\"p\"], para[\"lam_margin\"], para[\"lam_l1\"], para[\"lam_l2\"])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # 평가\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                y_pred = model(X_batch)\n",
    "                correct += (torch.argmax(y_pred, dim=1) == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "\n",
    "            test_acc = correct / total\n",
    "            print(f\"Epoch {epoch + 1}/{para['num_epoch']}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            early_stopping(test_acc)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    return early_stopping.best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_params = {\n",
    "    \"num_epoch\": 500,\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"lam_margin\": 0.01,\n",
    "    \"lam_l1\": 0.001,\n",
    "    \"lam_l2\": 0.001,\n",
    "    \"p\": 4,\n",
    "    \"patience\": 50,\n",
    "    \"min_delta\": 1e-4,\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"dropout_rate\": 0.3\n",
    "}\n",
    "\n",
    "# 학습 실행\n",
    "final_accuracy = R_MLR_with_FashionMNIST(x_train, y_train, x_test, y_test, fashion_params)\n",
    "print(f\"FashionMNIST Final Test Accuracy: {final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
